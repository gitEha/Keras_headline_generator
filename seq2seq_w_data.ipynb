{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ranet\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import pickle\n",
    "from xml.dom import minidom\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding\n",
    "\n",
    "from keras.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97415\n",
      "97415\n",
      "Headline: Kristiine linnaosa kortermajas süttis jõulupuu põlema, tuld võttis kogu toa sisustus \n",
      "\n",
      "Article:\n",
      " Täna pärastlõunal läks väga napilt, et ühe tavalise ning toreda pere jaoks oleks kogu aastavahetus võtnud hoopis kurva pöörde. Täna kell 16.19 sai häirekeskus teate, et Kristiines Sõpruse puiesteel on kortermaja ühes teise korruse korteris süttinud elutoas põlema nulg. Tulekahju sai alguse nulu külge riputatud ning põlema süüdatud säraküünaldest. Nulg oli kuiv ning kinnitatud selliselt, et puul vett polnud. Mõne hetkega süttis nulg ise põlema nagu säraküünal. Tuld võttis juba kogu toa sisustus. Tuleõnnetuse ajal oli kodus viis inimest, kolm täiskasvanut ja kaks last. Nähes, et tulekahju kustutamine pole enam võimalik, varjus kogu pere magamistuppa. Perel ei õnnestunud põgenemiseks magamistoa akent avada, pereisa lõi lapse kirjutuslauaga akna katki. Päästjate saabudes oli kogu pere roninud aknast kõrvaloleva autosalongi katusele. Päästjad aitasid nad sealt alla. Kaks meest viidi haiglasse, naine ja kaks last jäid koju. Päästjad kustutasid tulekahju paarikümne minutiga. Elutuba sai tulekahjustusi, kogu korter suitsukahjustusi. Päästeamet sõnab, et praeguseks hetkeks on inimestel kuused ja nulud kodus olnud juba oma paar nädalat. Nii vana jõulupuu on tõenäoliselt tubastes tingimustes juba kuivaks muutunud, liiati ilma veeta. Sellisele puule elusat tuld külge riputada ei tasu. Nagu näitas ka käesolev õnnetus, võib kuiv kuusk või nulg  hetkega süttida ning süüdata kogu elamise. Samuti on täna õhtul ja sellele järgneval ööl aasta ilutulestiku laskmise kõrghooaeg. Päästeamet tuletab meelde - ilutulestiku kasutamisele peab eelnema selle kasutusjuhendi lugemine. Kuidas lasta, kuhu kinnitada, kui kaugel hoonetest või metsast lasta. Kindlasti ei tohi läheduses olla inimesi või lemmikloomi, masinaid ja maju.\n"
     ]
    }
   ],
   "source": [
    "doc = minidom.parse('DATA/fixed_xml.xml')\n",
    "\n",
    "headlines_dom = doc.getElementsByTagName('ArticleHeadline')\n",
    "articles_dom = doc.getElementsByTagName('ArticleText')\n",
    "#cleantext = BeautifulSoup(raw_html, \"lxml\").text\n",
    "headlines = [headline.firstChild.data for headline in headlines_dom]\n",
    "articles = [BeautifulSoup(article.firstChild.data, \"lxml\").text for article in articles_dom]\n",
    "\n",
    "print(len(headlines))\n",
    "print(len(articles))\n",
    "\n",
    "print('Headline:', headlines[-1], '\\n')\n",
    "print('Article:\\n', articles[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5200\n",
      "['USAs New Yorgis suri täna Venemaa Föderatsiooni alaline esindaja\\xa0ÜRO juures Vitali Tšurkin, teatas Vene välisministeerium. Vitali Tšurkin suri tööpostil, kirjutas Interfax. Homme tähistanuks ta\\xa065aastast sünnipäeva. Tšurkin sündis Moskvas 1952. aastal. 1974. aastal lõpetas ta Moskva rahvusvaheliste suhete riikliku instituudi. Ta oli ajalooteaduste kandidaat. 1990. aastate algul töötas ta Vene välisministeeriumi informatsioonivalitsuse ülemana ja välisministri asetäitjana. 1994. aastast oli ta Venemaa suursaadik Belgia kuningriigis ning 1998. aastast Kanadas. Venemaa alaline esindaja ÜRO juures oli ta alates 2006. aastast. Tšurkini karjääris oli oluliseks hetkeks, kui ta noore Nõukogude diplomaadina andis Tšernobõli tuumakatastroofi kohta tunnistusi USA kongressi ees, kirjutas ERRi uudisteportaal. Tegemist oli ajaloos esimese korraga, kui Nõukogude Liidu ametiisik USA esindajatekoja komitee ees tunnistusi andis. Siis veel noor ja küllaltki vähetuntud Tšurkin valiti sellesse rolli tema\\xa0suurepärase inglise keele oskuse tõttu.', 'Suri Vene suursaadik ÜRO juures Vitali Tšurkin']\n"
     ]
    }
   ],
   "source": [
    "training_text = []\n",
    "training_data = []\n",
    "for i, (input_text, target_text) in enumerate(zip(articles, headlines)):\n",
    "    if input_text not in training_text:\n",
    "        training_text.append(input_text)\n",
    "        training_data.append([input_text.replace('\\t', '').replace('\\n', ''),\n",
    "                              target_text.replace('\\t', '').replace('\\n', '')])\n",
    "\n",
    "print(len(training_data))\n",
    "print(training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DATA/training_data.txt\", \"wb\") as fp:\n",
    "    pickle.dump(training_data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DATA/training_data.txt\", \"rb\") as fp:\n",
    "    training_data = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('DATA/articles_headlines.txt', 'w', encoding='utf8') as f:\n",
    "    for i, (input_text, target_text) in enumerate(training_data):\n",
    "        f.write(input_text.strip().replace('\\t', '').replace('\\n', '') + '\\t'\n",
    "                + target_text.strip().replace('\\t', '').replace('\\n', '')  + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 5200 5200\n",
      "Number of unique input tokens: 158\n",
      "Number of unique output tokens: 116\n",
      "Max sequence length for inputs: 29627\n",
      "Max sequence length for outputs: 182\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = 'DATA/articles_headlines2.txt'\n",
    "\n",
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "#with open(data_path, 'r', encoding='utf-8') as f:\n",
    "#    lines = f.read().split('END')\n",
    "    #lines2 = f.read().split('\\t')\n",
    "\n",
    "\n",
    "#for i, line in enumerate(lines[: min(num_samples, len(lines) - 1)]):\n",
    "#        input_text, target_text = line.split('\\t')\n",
    "for i, (input_text, target_text) in enumerate(training_data):\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = ' \\t ' + target_text + ' \\n '\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts), len(target_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true vocab size: 179058\n",
      " \t Suri Vene suursaadik ÜRO juures Vitali Tšurkin \n",
      " \n",
      "USAs New Yorgis suri täna Venemaa Föderatsiooni alaline esindaja ÜRO juures Vitali Tšurkin, teatas Vene välisministeerium. Vitali Tšurkin suri tööpostil, kirjutas Interfax. Homme tähistanuks ta 65aastast sünnipäeva. Tšurkin sündis Moskvas 1952. aastal. 1974. aastal lõpetas ta Moskva rahvusvaheliste suhete riikliku instituudi. Ta oli ajalooteaduste kandidaat. 1990. aastate algul töötas ta Vene välisministeeriumi informatsioonivalitsuse ülemana ja välisministri asetäitjana. 1994. aastast oli ta Venemaa suursaadik Belgia kuningriigis ning 1998. aastast Kanadas. Venemaa alaline esindaja ÜRO juures oli ta alates 2006. aastast. Tšurkini karjääris oli oluliseks hetkeks, kui ta noore Nõukogude diplomaadina andis Tšernobõli tuumakatastroofi kohta tunnistusi USA kongressi ees, kirjutas ERRi uudisteportaal. Tegemist oli ajaloos esimese korraga, kui Nõukogude Liidu ametiisik USA esindajatekoja komitee ees tunnistusi andis. Siis veel noor ja küllaltki vähetuntud Tšurkin valiti sellesse rolli tema suurepärase inglise keele oskuse tõttu.\n",
      "Headline: [    0     0     0     0     0     0    19   727   348  7153  2027   232\n",
      "  5472 20454    20]\n",
      "Text: [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0  1402  1036\n",
      "  6786   727    88   213  7042 21020   232  5472 20454   364   348  7558\n",
      "  5472 20454   727   411 23789   970  1269 20454   952  3038 14437    47\n",
      "  8594    47  1233     7  1510  9917  1704  3609  4389     7     9  5047\n",
      "  4390   938   892  1284     7   348  5098 32425     1  7854 40013  4115\n",
      "   457     9     7   213  7153  4648 23790    10  4289   457  9915   213\n",
      " 21020  1459  2027   232     9     7   377  1998   457 10474     9  3752\n",
      "  3815     5     7  1726   998 40014   342 40015   111 10475   162  8595\n",
      "   188   411  1138  2288   313     9  3875   343   783     5   998   415\n",
      "   162 23791  8233   188 10475   342    13    40   885     1  4116 39401\n",
      " 20454  1823  1856   933  1246  1658 23792   225]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50000\n",
    "text_len = 200\n",
    "headline_len = 15\n",
    "\n",
    "#def tokenization_processing(x_to_tok, x_val_to_tok):\n",
    "tokenizer = Tokenizer(num_words=vocab_size,\n",
    "                                   filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~',\n",
    "                                   lower=True,\n",
    "                                   split=\" \",\n",
    "                                   char_level=False,\n",
    "                                   oov_token='oov')\n",
    "tok_headline = target_texts\n",
    "tok_text = input_texts\n",
    "\n",
    "tokenizer.fit_on_texts(tok_headline + tok_text)\n",
    "true_vocab_size = len(tokenizer.word_index)\n",
    "print('true vocab size:',true_vocab_size)\n",
    "\n",
    "tok_headline = pad_sequences(tokenizer.texts_to_sequences(tok_headline), maxlen=headline_len)\n",
    "tok_text = pad_sequences(tokenizer.texts_to_sequences(tok_text), maxlen=text_len)\n",
    "\n",
    "print(target_texts[0])\n",
    "print(input_texts[0])\n",
    "print('Headline:',tok_headline[0])\n",
    "print('Text:',tok_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5200, 15)\n",
      "[    0     0     0     0     0     0    19   727   348  7153  2027   232\n",
      "  5472 20454    20]\n",
      "[0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 1.9000e+01\n",
      " 7.2700e+02 3.4800e+02 7.1530e+03 2.0270e+03 2.3200e+02 5.4720e+03\n",
      " 2.0454e+04 2.0000e+01 0.0000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(tok_headline.shape)    \n",
    "decoder_input_data = np.zeros((len(input_texts), headline_len), dtype='float32')\n",
    "for i, ex in enumerate(tok_headline):\n",
    "    for j, ex_val in enumerate(ex):\n",
    "        if j > 0:\n",
    "            decoder_input_data[i, j - 1] = tok_headline[i, j]\n",
    "print(tok_headline[0])\n",
    "print(decoder_input_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-b55024322f92>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtarget_token_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_characters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mencoder_input_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_texts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_encoder_seq_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_encoder_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mdecoder_input_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_texts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_decoder_seq_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_decoder_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdecoder_target_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_texts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_decoder_seq_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_decoder_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
    "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder_input_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-d1e6604f0879>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder_input_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder_input_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder_target_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder_input_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'encoder_input_data' is not defined"
     ]
    }
   ],
   "source": [
    "print(encoder_input_data.shape)\n",
    "print(decoder_input_data.shape)\n",
    "print(decoder_target_data.shape)\n",
    "\n",
    "print(encoder_input_data[0][1])\n",
    "print(decoder_input_data[0][1])\n",
    "print(decoder_target_data[0][1])\n",
    "\n",
    "print(''.join([list(target_token_index.keys())[list(target_token_index.values()).index(\n",
    "    np.argmax(i))] for i in decoder_input_data[0]]))\n",
    "print(''.join([list(target_token_index.keys())[list(target_token_index.values()).index(\n",
    "    np.argmax(i))] for i in decoder_target_data[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DATA/training_data_proc.txt\", \"wb\") as fp:\n",
    "    pickle.dump([target_texts, input_texts, decoder_input_data, tok_headline, tok_text, tokenizer], fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50000\n",
    "text_len = 200\n",
    "headline_len = 15\n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "\n",
    "with open(\"DATA/training_data_proc.txt\", \"rb\") as fp:\n",
    "    decoder_input_data, tok_headline, tok_text = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5200, 200)\n",
      "(5200, 15)\n",
      "(5200, 15)\n"
     ]
    }
   ],
   "source": [
    "num_encoder_tokens = text_len\n",
    "num_decoder_tokens = headline_len\n",
    "\n",
    "tok_text  = tok_text.reshape(5200, 200)\n",
    "tok_headline = tok_headline.reshape(5200, 15)\n",
    "decoder_input_data = decoder_input_data.reshape(5200, 15)\n",
    "#tok_text = np.expand_dims(tok_text, -1)\n",
    "#tok_headline = np.expand_dims(tok_headline, -1)\n",
    "#decoder_input_data = np.expand_dims(decoder_input_data, -1)\n",
    "print(tok_text.shape)\n",
    "print(tok_headline.shape)\n",
    "print(decoder_input_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "x = Embedding(vocab_size, latent_dim)(encoder_inputs)\n",
    "x, state_h, state_c = LSTM(latent_dim,\n",
    "                           return_state=True)(x)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "x = Embedding(vocab_size, latent_dim)(decoder_inputs)\n",
    "x = LSTM(latent_dim, return_sequences=True)(x, initial_state=encoder_states)\n",
    "decoder_outputs = Dense(vocab_size, activation='softmax')(x)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(features, labels, batch_size):\n",
    "    # Create empty arrays to contain batch of features and labels\n",
    "    batch_features_enc = np.zeros((batch_size, num_encoder_tokens))\n",
    "    batch_features_dec = np.zeros((batch_size, num_decoder_tokens))\n",
    "    batch_labels = np.zeros((batch_size, num_decoder_tokens, vocab_size))\n",
    "    #import pdb;pdb.set_trace()\n",
    "    while True:\n",
    "        for i in range(batch_size):\n",
    "#             import pdb;pdb.set_trace()\n",
    "#             batch_features_enc[i] = features[0][i]\n",
    "#             batch_features_dec[i] = features[1][i]\n",
    "#             print(batch_features_enc.shape)\n",
    "#             print(batch_features_dec.shape)\n",
    "#             print(batch_labels.shape)\n",
    "            batch_labels[i] = to_categorical(labels[i], num_classes=vocab_size)\n",
    "#         print(batch_labels.shape)\n",
    "        yield [batch_features_enc, batch_features_dec], batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ranet\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Ranet\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., steps_per_epoch=81.25, epochs=100)`\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "82/81 [==============================] - 53s 649ms/step - loss: 3.8095\n",
      "Epoch 2/100\n",
      "82/81 [==============================] - 50s 604ms/step - loss: 3.5411\n",
      "Epoch 3/100\n",
      "82/81 [==============================] - 50s 604ms/step - loss: 3.3431\n",
      "Epoch 4/100\n",
      "82/81 [==============================] - 50s 611ms/step - loss: 3.2233\n",
      "Epoch 5/100\n",
      "82/81 [==============================] - 50s 609ms/step - loss: 3.1463\n",
      "Epoch 6/100\n",
      "82/81 [==============================] - 50s 609ms/step - loss: 3.0868\n",
      "Epoch 7/100\n",
      "82/81 [==============================] - 50s 614ms/step - loss: 3.0414\n",
      "Epoch 8/100\n",
      "82/81 [==============================] - 49s 602ms/step - loss: 3.0044\n",
      "Epoch 9/100\n",
      "82/81 [==============================] - 49s 596ms/step - loss: 2.9722\n",
      "Epoch 10/100\n",
      "82/81 [==============================] - 50s 605ms/step - loss: 2.9457\n",
      "Epoch 11/100\n",
      "82/81 [==============================] - 49s 600ms/step - loss: 2.9199\n",
      "Epoch 12/100\n",
      "82/81 [==============================] - 49s 602ms/step - loss: 2.8969\n",
      "Epoch 13/100\n",
      "82/81 [==============================] - 49s 600ms/step - loss: 2.8771\n",
      "Epoch 14/100\n",
      "82/81 [==============================] - 50s 604ms/step - loss: 2.8577\n",
      "Epoch 15/100\n",
      "82/81 [==============================] - 49s 601ms/step - loss: 2.8391\n",
      "Epoch 16/100\n",
      "82/81 [==============================] - 49s 602ms/step - loss: 2.8390\n",
      "Epoch 17/100\n",
      "82/81 [==============================] - 49s 601ms/step - loss: 2.8078\n",
      "Epoch 18/100\n",
      "82/81 [==============================] - 49s 600ms/step - loss: 2.7916\n",
      "Epoch 19/100\n",
      "82/81 [==============================] - 49s 603ms/step - loss: 2.7778\n",
      "Epoch 20/100\n",
      "82/81 [==============================] - 49s 602ms/step - loss: 2.7606\n",
      "Epoch 21/100\n",
      "82/81 [==============================] - 49s 600ms/step - loss: 2.7494\n",
      "Epoch 22/100\n",
      "82/81 [==============================] - 49s 601ms/step - loss: 2.7218\n",
      "Epoch 23/100\n",
      "82/81 [==============================] - 49s 597ms/step - loss: 2.7094\n",
      "Epoch 24/100\n",
      "82/81 [==============================] - 49s 598ms/step - loss: 2.7893\n",
      "Epoch 25/100\n",
      "82/81 [==============================] - 50s 605ms/step - loss: 2.8264\n",
      "Epoch 26/100\n",
      "82/81 [==============================] - 49s 602ms/step - loss: 2.7019\n",
      "Epoch 27/100\n",
      "82/81 [==============================] - 49s 602ms/step - loss: 2.6944\n",
      "Epoch 28/100\n",
      "82/81 [==============================] - 49s 599ms/step - loss: 2.6746\n",
      "Epoch 29/100\n",
      "82/81 [==============================] - 49s 602ms/step - loss: 2.6820\n",
      "Epoch 30/100\n",
      "82/81 [==============================] - 49s 596ms/step - loss: 2.6489\n",
      "Epoch 31/100\n",
      "82/81 [==============================] - 50s 605ms/step - loss: 2.6447\n",
      "Epoch 32/100\n",
      "82/81 [==============================] - 49s 599ms/step - loss: 2.6249\n",
      "Epoch 33/100\n",
      "82/81 [==============================] - 50s 609ms/step - loss: 2.7429\n",
      "Epoch 34/100\n",
      "82/81 [==============================] - 50s 616ms/step - loss: 2.6422\n",
      "Epoch 35/100\n",
      "82/81 [==============================] - 49s 601ms/step - loss: 2.6243\n",
      "Epoch 36/100\n",
      "82/81 [==============================] - 50s 610ms/step - loss: 2.6095\n",
      "Epoch 37/100\n",
      "82/81 [==============================] - 49s 598ms/step - loss: 2.5866\n",
      "Epoch 38/100\n",
      "82/81 [==============================] - 49s 597ms/step - loss: 2.5782\n",
      "Epoch 39/100\n",
      "82/81 [==============================] - 49s 598ms/step - loss: 2.5706\n",
      "Epoch 40/100\n",
      "82/81 [==============================] - 49s 599ms/step - loss: 2.6276\n",
      "Epoch 41/100\n",
      "82/81 [==============================] - 49s 601ms/step - loss: 2.5588\n",
      "Epoch 42/100\n",
      "82/81 [==============================] - 49s 599ms/step - loss: 2.5529\n",
      "Epoch 43/100\n",
      "82/81 [==============================] - 49s 601ms/step - loss: 2.5474\n",
      "Epoch 44/100\n",
      "82/81 [==============================] - 49s 597ms/step - loss: 2.5422\n",
      "Epoch 45/100\n",
      "82/81 [==============================] - 49s 596ms/step - loss: 2.5372\n",
      "Epoch 46/100\n",
      "82/81 [==============================] - 49s 597ms/step - loss: 2.5325\n",
      "Epoch 47/100\n",
      "82/81 [==============================] - 49s 596ms/step - loss: 2.5286\n",
      "Epoch 48/100\n",
      "82/81 [==============================] - 49s 598ms/step - loss: 2.6698\n",
      "Epoch 49/100\n",
      "82/81 [==============================] - 49s 599ms/step - loss: 2.5522\n",
      "Epoch 50/100\n",
      "82/81 [==============================] - 49s 598ms/step - loss: 2.5439\n",
      "Epoch 51/100\n",
      "82/81 [==============================] - 49s 599ms/step - loss: 2.5381\n",
      "Epoch 52/100\n",
      "82/81 [==============================] - 49s 599ms/step - loss: 2.5330\n",
      "Epoch 53/100\n",
      "82/81 [==============================] - 49s 595ms/step - loss: 2.5263\n",
      "Epoch 54/100\n",
      "82/81 [==============================] - 49s 600ms/step - loss: 2.5107\n",
      "Epoch 55/100\n",
      "82/81 [==============================] - 49s 600ms/step - loss: 2.5066\n",
      "Epoch 56/100\n",
      "82/81 [==============================] - 49s 598ms/step - loss: 2.5033\n",
      "Epoch 57/100\n",
      "82/81 [==============================] - 50s 611ms/step - loss: 2.5002\n",
      "Epoch 58/100\n",
      "82/81 [==============================] - 50s 607ms/step - loss: 2.4973\n",
      "Epoch 59/100\n",
      "82/81 [==============================] - 50s 606ms/step - loss: 2.4947\n",
      "Epoch 60/100\n",
      "82/81 [==============================] - 50s 611ms/step - loss: 2.5833\n",
      "Epoch 61/100\n",
      "82/81 [==============================] - 50s 614ms/step - loss: 2.5905\n",
      "Epoch 62/100\n",
      "82/81 [==============================] - 50s 607ms/step - loss: 2.5070\n",
      "Epoch 63/100\n",
      "82/81 [==============================] - 50s 612ms/step - loss: 2.5021\n",
      "Epoch 64/100\n",
      "82/81 [==============================] - 50s 610ms/step - loss: 2.4990\n",
      "Epoch 65/100\n",
      "82/81 [==============================] - 50s 605ms/step - loss: 2.4962\n",
      "Epoch 66/100\n",
      "82/81 [==============================] - 50s 609ms/step - loss: 2.4935\n",
      "Epoch 67/100\n",
      "82/81 [==============================] - 49s 603ms/step - loss: 2.4911\n",
      "Epoch 68/100\n",
      "82/81 [==============================] - 49s 603ms/step - loss: 2.4890\n",
      "Epoch 69/100\n",
      "82/81 [==============================] - 50s 604ms/step - loss: 2.5227\n",
      "Epoch 70/100\n",
      "82/81 [==============================] - 50s 608ms/step - loss: 2.4846\n",
      "Epoch 71/100\n",
      "82/81 [==============================] - 50s 609ms/step - loss: 2.4818\n",
      "Epoch 72/100\n",
      "82/81 [==============================] - 49s 602ms/step - loss: 2.4798\n",
      "Epoch 73/100\n",
      "82/81 [==============================] - 49s 598ms/step - loss: 2.4778\n",
      "Epoch 74/100\n",
      "82/81 [==============================] - 49s 593ms/step - loss: 2.4761\n",
      "Epoch 75/100\n",
      "82/81 [==============================] - 49s 594ms/step - loss: 2.4746\n",
      "Epoch 76/100\n",
      "82/81 [==============================] - 49s 595ms/step - loss: 2.4737\n",
      "Epoch 77/100\n",
      "82/81 [==============================] - 49s 594ms/step - loss: 2.4727\n",
      "Epoch 78/100\n",
      "82/81 [==============================] - 50s 606ms/step - loss: 2.4708\n",
      "Epoch 79/100\n",
      "82/81 [==============================] - 50s 610ms/step - loss: 2.4696\n",
      "Epoch 80/100\n",
      "82/81 [==============================] - 50s 614ms/step - loss: 2.4685\n",
      "Epoch 81/100\n",
      "82/81 [==============================] - 51s 624ms/step - loss: 2.4665\n",
      "Epoch 82/100\n",
      "82/81 [==============================] - 51s 618ms/step - loss: 2.4648\n",
      "Epoch 83/100\n",
      "82/81 [==============================] - 51s 618ms/step - loss: 2.4644\n",
      "Epoch 84/100\n",
      "82/81 [==============================] - 50s 612ms/step - loss: 2.4632\n",
      "Epoch 85/100\n",
      "82/81 [==============================] - 50s 613ms/step - loss: 2.4573\n",
      "Epoch 86/100\n",
      "82/81 [==============================] - 50s 613ms/step - loss: 2.4554\n",
      "Epoch 87/100\n",
      "82/81 [==============================] - 51s 618ms/step - loss: 2.4548\n",
      "Epoch 88/100\n",
      "82/81 [==============================] - 50s 615ms/step - loss: 2.4541\n",
      "Epoch 89/100\n",
      "82/81 [==============================] - 51s 616ms/step - loss: 2.4526\n",
      "Epoch 90/100\n",
      "82/81 [==============================] - 50s 614ms/step - loss: 2.4518\n",
      "Epoch 91/100\n",
      "82/81 [==============================] - 50s 612ms/step - loss: 2.4514\n",
      "Epoch 92/100\n",
      "82/81 [==============================] - 50s 611ms/step - loss: 2.9309\n",
      "Epoch 93/100\n",
      "82/81 [==============================] - 50s 612ms/step - loss: 2.9446\n",
      "Epoch 94/100\n",
      "82/81 [==============================] - 50s 609ms/step - loss: 2.8454\n",
      "Epoch 95/100\n",
      "82/81 [==============================] - 50s 612ms/step - loss: 2.8269\n",
      "Epoch 96/100\n",
      "82/81 [==============================] - 51s 616ms/step - loss: 2.7932\n",
      "Epoch 97/100\n",
      "82/81 [==============================] - 50s 614ms/step - loss: 2.7653\n",
      "Epoch 98/100\n",
      "82/81 [==============================] - 50s 606ms/step - loss: 2.7278\n",
      "Epoch 99/100\n",
      "82/81 [==============================] - 50s 609ms/step - loss: 2.7140\n",
      "Epoch 100/100\n",
      "82/81 [==============================] - 50s 606ms/step - loss: 2.7007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24c9525fc18>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run training\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "# model.fit([tok_text, decoder_input_data], tok_headline,\n",
    "#           batch_size=batch_size,\n",
    "#           epochs=epochs,\n",
    "#           validation_split=0.2)\n",
    "\n",
    "model.fit_generator(generator([tok_text, decoder_input_data], tok_headline, batch_size),\n",
    "                    steps_per_epoch=tok_text.shape[0] / batch_size, nb_epoch=100)\n",
    "\n",
    "# Save model\n",
    "#model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('DATA/100_2.7007.h5')\n",
    "model.save_weights('DATA/100_2.7007_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer lstm_19 expects 5 inputs, but it received 3 input tensors. Input received: [<tf.Tensor 'input_25:0' shape=(?, ?) dtype=float32>, <tf.Tensor 'input_26:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'input_27:0' shape=(?, 256) dtype=float32>]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-51e07e8ae6c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mdecoder_state_input_c\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mdecoder_states_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdecoder_state_input_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_state_input_c\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mdecoder_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_c\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder_lstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecoder_states_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mdecoder_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstate_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_c\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    535\u001b[0m             \u001b[0moriginal_input_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfull_input_spec\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 537\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    538\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moriginal_input_spec\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    571\u001b[0m                 \u001b[1;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m                 \u001b[1;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 573\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    574\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m                 \u001b[1;31m# Collect input shapes to build layer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    458\u001b[0m                              \u001b[1;34m'but it received '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m                              \u001b[1;34m' input tensors. Input received: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 460\u001b[1;33m                              str(inputs))\n\u001b[0m\u001b[0;32m    461\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0minput_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mspec\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Layer lstm_19 expects 5 inputs, but it received 3 input tensors. Input received: [<tf.Tensor 'input_25:0' shape=(?, ?) dtype=float32>, <tf.Tensor 'input_26:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'input_27:0' shape=(?, 256) dtype=float32>]"
     ]
    }
   ],
   "source": [
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs,[decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Oh no!\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: He ran.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: I know.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: I lost.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: I quit.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: I'm 19.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: I'm 19.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: I'm 19.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Listen.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Listen.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: We try.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Why me?\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Why me?\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Ask Tom.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Be cool.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Be cool.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Beat it.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Come on!\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Get out!\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Get out!\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Go away.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Go away.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Goodbye!\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Goodbye!\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Goodbye!\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Hang on!\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Hang on.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: He came.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: He runs.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Help me!\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Help me!\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Help me!\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Help me.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Hold it!\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Hold it!\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: I agree.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: I agree.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: I'm fat.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: I'm hit!\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: I'm shy.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: I'm shy.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: I'm wet.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: It's OK.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: It's OK.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: It's me!\n",
      "Decoded sentence: Ma ei teenud seda selleksid lähale.\n",
      "\n",
      "-\n",
      "Input sentence: It's me!\n",
      "Decoded sentence: Ma ei teenud seda selleksid lähale.\n",
      "\n",
      "-\n",
      "Input sentence: Join us.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Keep it.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Keep it.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Keep it.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Keep it.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Kiss me.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Kiss me.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Me, too.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Open up.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Open up.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Perfect!\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Perfect!\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: See you.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Show me.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Take it.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Take it.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Take it.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Tell me.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Wake up!\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Wash up.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Wash up.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Wash up.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Why not?\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: You run.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: Have fun.\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: How cute!\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n",
      "-\n",
      "Input sentence: How cute!\n",
      "Decoded sentence: Ma ei saa sinu enda.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
